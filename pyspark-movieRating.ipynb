{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001B0AC481518>\n"
     ]
    }
   ],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create my_spark\n",
    "my_spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Print my_spark\n",
    "print(my_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "# Print the tables in the catalog\n",
    "print(my_spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[0: double]\n",
      "[Table(name='temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create pd_temp\n",
    "pd_temp = pd.DataFrame(np.random.random(10))\n",
    "\n",
    "# Create spark_temp from pd_temp\n",
    "spark_temp = my_spark.createDataFrame(pd_temp)\n",
    "\n",
    "# Examine the tables in the catalog\n",
    "print(spark_temp)\n",
    "\n",
    "# Add spark_temp to the catalog\n",
    "spark_temp.createOrReplaceTempView('temp')\n",
    "\n",
    "# Examine the tables in the catalog again\n",
    "print(my_spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|    296|   5.0|1147880044|\n",
      "|     1|    306|   3.5|1147868817|\n",
      "|     1|    307|   5.0|1147868828|\n",
      "|     1|    665|   5.0|1147878820|\n",
      "|     1|    899|   3.5|1147868510|\n",
      "|     1|   1088|   4.0|1147868495|\n",
      "|     1|   1175|   3.5|1147868826|\n",
      "|     1|   1217|   3.5|1147878326|\n",
      "|     1|   1237|   5.0|1147868839|\n",
      "|     1|   1250|   4.0|1147868414|\n",
      "|     1|   1260|   3.5|1147877857|\n",
      "|     1|   1653|   4.0|1147868097|\n",
      "|     1|   2011|   2.5|1147868079|\n",
      "|     1|   2012|   2.5|1147868068|\n",
      "|     1|   2068|   2.5|1147869044|\n",
      "|     1|   2161|   3.5|1147868609|\n",
      "|     1|   2351|   4.5|1147877957|\n",
      "|     1|   2573|   4.0|1147878923|\n",
      "|     1|   2632|   5.0|1147878248|\n",
      "|     1|   2692|   5.0|1147869100|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Don't change this file path\n",
    "file_path = \"C:/Users/laayt/Downloads/movie_pyspark/ratings.csv\"\n",
    "\n",
    "# Read in the airports data\n",
    "ratings =spark.read.csv(file_path,header=True)\n",
    "\n",
    "# Show the data\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162541"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.select(\"userId\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59047"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.select(\"movieId\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALS(Alternative Least Square Algorithm) works well with sparse datasets. Let's see how much of the ratings matrix is actually empty.\n",
    "\n",
    "sparsity is calculated by the number of cells in a matrix that contain a rating divided by the total number of values that matrix could hold given the number of users and items (movies). \n",
    "\n",
    "In other words, dividing the number of ratings present in the matrix by the product of users and movies in the matrix and subtracting that from 1 will give us the sparsity or the percentage of the ratings matrix that is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratings dataframe is 99.74% empty.\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of ratings in the dataset\n",
    "numerator = ratings.select(\"rating\").count()\n",
    "\n",
    "# Count the number of distinct userIds and distinct movieIds\n",
    "num_users = ratings.select(\"userId\").distinct().count()\n",
    "num_movies = ratings.select(\"movieId\").distinct().count()\n",
    "\n",
    "# Set the denominator equal to the number of users multiplied by the number of movies\n",
    "denominator = num_users * num_movies\n",
    "\n",
    "# Divide the numerator by the denominator\n",
    "sparsity = (1.0 - (numerator *1.0)/denominator)*100\n",
    "print(\"The ratings dataframe is\", \"%.2f\" % sparsity + \"% empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|    296|   5.0|\n",
      "|     1|    306|   3.5|\n",
      "|     1|    307|   5.0|\n",
      "|     1|    665|   5.0|\n",
      "|     1|    899|   3.5|\n",
      "|     1|   1088|   4.0|\n",
      "|     1|   1175|   3.5|\n",
      "|     1|   1217|   3.5|\n",
      "|     1|   1237|   5.0|\n",
      "|     1|   1250|   4.0|\n",
      "|     1|   1260|   3.5|\n",
      "|     1|   1653|   4.0|\n",
      "|     1|   2011|   2.5|\n",
      "|     1|   2012|   2.5|\n",
      "|     1|   2068|   2.5|\n",
      "|     1|   2161|   3.5|\n",
      "|     1|   2351|   4.5|\n",
      "|     1|   2573|   4.0|\n",
      "|     1|   2632|   5.0|\n",
      "|     1|   2692|   5.0|\n",
      "+------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|    296|   5.0|\n",
      "|     1|    306|   3.5|\n",
      "|     1|    307|   5.0|\n",
      "|     1|    665|   5.0|\n",
      "|     1|    899|   3.5|\n",
      "|     1|   1088|   4.0|\n",
      "|     1|   1175|   3.5|\n",
      "|     1|   1217|   3.5|\n",
      "|     1|   1237|   5.0|\n",
      "|     1|   1250|   4.0|\n",
      "|     1|   1260|   3.5|\n",
      "|     1|   1653|   4.0|\n",
      "|     1|   2011|   2.5|\n",
      "|     1|   2012|   2.5|\n",
      "|     1|   2068|   2.5|\n",
      "|     1|   2161|   3.5|\n",
      "|     1|   2351|   4.5|\n",
      "|     1|   2573|   4.0|\n",
      "|     1|   2632|   5.0|\n",
      "|     1|   2692|   5.0|\n",
      "+------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+-----+\n",
      "|userId|count|\n",
      "+------+-----+\n",
      "|   148|  132|\n",
      "|   463|   21|\n",
      "|   471|   39|\n",
      "|   496|   31|\n",
      "|   833|  158|\n",
      "|  1088|   36|\n",
      "|  1238|  150|\n",
      "|  1342|  390|\n",
      "|  1580|   22|\n",
      "|  1591|   26|\n",
      "|  1645|  101|\n",
      "|  1829|  342|\n",
      "|  1959|  103|\n",
      "|  2122|   61|\n",
      "|  2142|   86|\n",
      "|  2366|   24|\n",
      "|  2659|   30|\n",
      "|  2866|   77|\n",
      "|  3175|   80|\n",
      "|  3749|   51|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the requisite packages\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# View the ratings dataset\n",
    "ratings.show()\n",
    "\n",
    "# Filter to show only userIds less than 100\n",
    "ratings.filter(col(\"userId\") < 100).show()\n",
    "\n",
    "# Group data by userId, count song plays\n",
    "ratings.groupBy(\"userId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg num ratings per movie: \n",
      "+-----------------+\n",
      "|       avg(count)|\n",
      "+-----------------+\n",
      "|423.3931444442563|\n",
      "+-----------------+\n",
      "\n",
      "User with the fewest ratings: \n",
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|        20|\n",
      "+----------+\n",
      "\n",
      "Avg num ratings per user: \n",
      "+------------------+\n",
      "|        avg(count)|\n",
      "+------------------+\n",
      "|153.80793153727367|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max, avg\n",
    "\n",
    "# Avg num ratings per movie\n",
    "print(\"Avg num ratings per movie: \")\n",
    "ratings.groupBy(\"movieId\").count().select(avg(\"count\")).show()\n",
    "\n",
    "# Min num ratings for user\n",
    "print(\"User with the fewest ratings: \")\n",
    "ratings.groupBy(\"userId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num ratings per users\n",
    "print(\"Avg num ratings per user: \")\n",
    "ratings.groupBy(\"userId\").count().select(avg(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark's implementation of ALS requires that movieIds and userIds be provided as integer datatypes. Many datasets need to be prepared accordingly in order for them to function properly with Spark. A common issue is that Spark thinks numbers are strings, and vice versa.\n",
    "\n",
    "Here, you'll use the .cast() method to address these types of problems. Let's take a look at the schema of the dataset to ensure it's in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use .printSchema() to see the datatypes of the ratings dataset\n",
    "ratings.printSchema()\n",
    "\n",
    "# Tell Spark to convert the columns to the proper data types\n",
    "ratings = ratings.select(ratings.userId.cast(\"integer\"), ratings.movieId.cast(\"integer\"), ratings.rating.cast(\"double\"))\n",
    "\n",
    "# Call .printSchema() again to confirm the columns are now in the correct format\n",
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating|prediction|\n",
      "+------+-------+------+----------+\n",
      "|114572|    148|   2.0|  2.512945|\n",
      "| 47989|    148|   2.0| 3.2016191|\n",
      "| 99010|    148|   3.0| 2.3791869|\n",
      "| 57779|    148|   2.0| 2.7970402|\n",
      "| 35969|    148|   2.0| 2.8812423|\n",
      "| 64112|    148|   3.0| 3.1633785|\n",
      "|128124|    148|   3.0| 3.1427312|\n",
      "|111567|    148|   3.0| 2.9362452|\n",
      "| 66440|    148|   2.5|  3.063164|\n",
      "| 69123|    148|   4.5| 2.3728032|\n",
      "| 49403|    148|   2.0| 1.2959298|\n",
      "|104825|    148|   4.0| 3.1629643|\n",
      "| 29213|    148|   5.0|  2.715926|\n",
      "|138552|    148|   4.0| 3.6291873|\n",
      "|  7223|    148|   3.0| 2.5713556|\n",
      "| 88277|    148|   2.0| 2.3611398|\n",
      "| 74794|    148|   3.0| 2.5478253|\n",
      "|110863|    148|   3.0| 2.9120758|\n",
      "|112701|    148|   4.0| 2.7818732|\n",
      "| 54751|    148|   3.0|  2.936366|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the ratings dataframe into training and test data\n",
    "(training_data, test_data) = ratings.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Set the ALS hyperparameters\n",
    "from pyspark.ml.recommendation import ALS\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", rank = 10, maxIter = 15, regParam = .1,\n",
    "          coldStartStrategy=\"drop\", nonnegative = True, implicitPrefs = False)\n",
    "\n",
    "# Fit the mdoel to the training_data\n",
    "model = als.fit(training_data)\n",
    "\n",
    "# Generate predictions on the test_data\n",
    "test_predictions = model.transform(test_data)\n",
    "test_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse\n",
      "rating\n",
      "prediction\n",
      "0.8096516314077491\n"
     ]
    }
   ],
   "source": [
    "# Import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Complete the evaluator code\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "# Extract the 3 parameters\n",
    "print(evaluator.getMetricName())\n",
    "print(evaluator.getLabelCol())\n",
    "print(evaluator.getPredictionCol()) \n",
    "\n",
    "# Evaluate the \"predictions\" dataframe\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "\n",
    "# Print the RMSE\n",
    "print (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae\n",
      "rating\n",
      "prediction\n",
      "0.6264011030776535\n"
     ]
    }
   ],
   "source": [
    "# Complete the evaluator code\n",
    "evaluator1 = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "# Extract the 3 parameters\n",
    "print(evaluator1.getMetricName())\n",
    "print(evaluator1.getLabelCol())\n",
    "print(evaluator1.getPredictionCol()) \n",
    "\n",
    "# Evaluate the \"predictions\" dataframe\n",
    "MAE = evaluator1.evaluate(test_predictions)\n",
    "\n",
    "print(MAE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
